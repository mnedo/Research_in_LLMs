## Исходный код ##
[Notebook with calculations]()

## Границы экспеирементов ##
В рамках домашнего задания был поставлен ряд экспериментов с алгоритмами RL в окружении CartPole из gymnasium. Модель должна делать выбор между движениями тележки, не уронив шеста. Каждый эпизод ограничен 500 командами.

Политика - FCN модель. На вход подается 4 числа - параметры состояния среды, на выходе модель выдает оценку каждому действию - логит вероятности выбора каждого действия.
![Архитектра PolicyModel]()

## Алгоритмы RL ##
При обучении алгоритма использовались:
* Vanila Policy Gardient алгоритм. $loss = log{\pi (a_t | s_t )} \cdot G_t$
* Policy Gardient with baseline. $loss = log{\pi (a_t | s_t )} \cdot (G_t- b)$
  *  $Baseline = mean(G_t)$
  *  $Baseline = ValueModel(s_t)$
  *  $Baseline = mean(G_i), i \in [1, ..., t-1, t+1, ...]$

Для начального сравнения каждая из модель начинала учиться с одинаковым seed и просматривала $5000$ эпизизодов, с $learinig \ rate = 10^{-3}$ и $\gamma = 1 - 10^{-2}$:

![Результаты]()
По резльутатам запуска:
| RL Алгоритм  | Количество эпизода до плато | Средняя награда за последине 100 эпизодов |
| :-: | :-: | :-: |
| VPG |  |  |
| PG, b = mean |  |  |
| PG, b = ValueModel |  |  |
| PG, RLOO |  |  |

Выводы: ...

### Также были проведены исследования с изменением гипераметров и добавлением регулярзиации на энтропию ### 

Таблица с результатами добавления энтропии
| VPG  | PG, b = mean | PG, b = ValueModel | PG, RLOO |
| :-: | :-: | :-: |  :-: |
| ![]() | ![]() | ![]() | ![]() |

Выводы: ...

Таблица с изменением $learing \ rate$
| VPG  | PG, b = mean | PG, b = ValueModel | PG, RLOO |
| :-: | :-: | :-: |  :-: |
| ![]() | ![]() | ![]() | ![]() |

Выводы: ...

Таблица с изменением $\gamma$
| VPG  | PG, b = mean | PG, b = ValueModel | PG, RLOO |
| :-: | :-: | :-: |  :-: |
| ![]() | ![]() | ![]() | ![]() |

Выводы: ...

## BEHAVIOUR CLONING ##
